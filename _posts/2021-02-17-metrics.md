---
id: 12971
title: Machine Learning Metrics
date: 2020-05-03
author: taimane
layout: post
permalink: /machine-learning/metrics
published: false
image: 
categories: 
   - r
tags:
   - pdf
---
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

_Table of Contents:_

- [Regression metrics notation](#regression-metrics-notation)
- [Absolute errors for regression](#absolute-errors-for-regression)
  - [MSE: mean square error](#mse-mean-square-error)
  - [RMSE: root mean square error](#rmse-root-mean-square-error)
  - [MAE: Mean Absolute Error](#mae-mean-absolute-error)
- [Relative errors for regression:](#relative-errors-for-regression)
  - [R-squared:](#r-squared)
  - [MSPE: mean square percentage error](#mspe-mean-square-percentage-error)
  - [MSE: mean absolute percentage error](#mse-mean-absolute-percentage-error)
  - [RMSLE: root mean square logarithmic error](#rmsle-root-mean-square-logarithmic-error)
- [Classification metrics notation](#classification-metrics-notation)
  - [Accuracy](#accuracy)
  - [Logarithmic loss (logloss)](#logarithmic-loss-logloss)
    - [Binary](#binary)
    - [Multiclass:](#multiclass)
  - [AUC (ROC)](#auc-roc)
  - [Quadratic Weighted Kappa](#quadratic-weighted-kappa)
- [Review of Scikit learn metrics methods](#review-of-scikit-learn-metrics-methods)
  - [Classification](#classification)
  - [Regression](#regression)


We will try to deal with different [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html){:rel="nofollow"} and to provide some examples. In particular we will just analyse regression and classification metrics most notable cases.

## Regression metrics notation

$N-$ number of objects

$y \in \mathbb{R}^{N}-$ target values

$\hat{y} \in \mathbb{R}^{N}-$ predictions

$\hat{y}_{i} \in \mathbb{R}-$ prediction for i-th object

$y_{i} \in \mathbb{R}-$ target for $\mathrm{i}$ -th object


## Absolute errors for regression

### MSE: mean square error

$$
\mathrm{MSE}=\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}
$$

**Example**: Minimize the MSE error

To minimize the MSE error we need to find the constant value of $c$ such that the MSE is minimal.


$$
\begin{aligned}\min _{c} \operatorname{MSE}(c)=\frac{1}{N} \sum_{i=1}^{N}\left(c-y_{i}\right)^{2}\end{aligned}
$$


You can set MSE derivative to 0 to get the optimal constant for MSE metric is **target mean value**.

Example: Where is the minimum error

```python
N = 5
y_true = np.random.randint(low=1, high=100, size=N)
print("array:", y_true, "mean:",  np.mean(y_true))
#MSE:   
from sklearn.metrics import mean_squared_error
x = np.arange(start=0, stop=100, step=.1)
y = np.empty_like(x)
for idx, xi in enumerate(x):
    y_pred = [xi]*N
    y[idx] = mean_squared_error(y_true, y_pred)

%matplotlib inline
from matplotlib import pyplot as plt

fig, ax = plt.subplots(figsize=(7, 5))
ax.set_xlabel('x')
ax.set_ylabel('Error y', labelpad=18)
ax.set_title("Mean Squared Error")
pic = ax.plot(x,y)
```

![mse](/wp-content/uploads/2021/02/mse1.png)

Note in here the minimum is at values mean 32.4.

### RMSE: root mean square error

$$\operatorname{RMSE}=\sqrt{\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}}=\sqrt{\mathrm{MSE}}$$

The minimum is equivalent as in the previous case, only we take the root now.

```python
N = 5
y_true = np.random.randint(low=1, high=100, size=N)
print(y_true, y_true.mean())
#MRSE:   
from sklearn.metrics import mean_squared_error
x = np.arange(start=0, stop=100, step=.1)
y = np.empty_like(x)
for idx, xi in enumerate(x):
    y_pred = [xi]*N
    y[idx] = mean_squared_error(y_true, y_pred)**0.5

%matplotlib inline
from matplotlib import pyplot as plt

fig, ax = plt.subplots(figsize=(7, 5))
ax.set_xlabel('x')
ax.set_ylabel('Error y', labelpad=18)
pic = ax.plot(x,y)
```

![mrse](/wp-content/uploads/2021/02/rmse1.png)

### MAE: Mean Absolute Error
$$
\mathrm{MAE}=\frac{1}{N} \sum_{i=1}^{N}\left|y_{i}-\hat{y}_{i}\right|
$$

```python
N = 5
y_true = np.random.randint(low=1, high=100, size=N)
print("array:", y_true, "median:",  np.median(y_true))
#MRSE:   
from sklearn.metrics import mean_absolute_error
x = np.arange(start=0, stop=100, step=.1)
y = np.empty_like(x)
for idx, xi in enumerate(x):
    y_pred = [xi]*N
    y[idx] = mean_absolute_error(y_true, y_pred)

%matplotlib inline
from matplotlib import pyplot as plt

fig, ax = plt.subplots(figsize=(7, 5))
ax.set_xlabel('x')
ax.set_ylabel('Error y', labelpad=18)
ax.set_title("Mean Absolute Error")
pic = ax.plot(x,y)
```

![mae](/wp-content/uploads/2021/02/mae1.png)


> If you have outliers in the data use MAE.


## Relative errors for regression:

### R-squared:
$$
R^{2}=1-\frac{\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}}{\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\bar{y}\right)^{2}}=1-\frac {\operatorname{MSE}}{\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\bar{y}\right)^{2}}
$$

where: 
$$\bar{y}=\frac{1}{N} \sum_{i=1}^{N} y_{i}$$



### MSPE: mean square percentage error

$$\mathrm{MSPE}=\frac{100 \%}{N} \sum_{i=1}^{N}\left(\frac{y_{i}-\hat{y}_{i}}{y_{i}}\right)^{2}$$


### MSE: mean absolute percentage error

$$\mathrm{MAPE}=\frac{100 \%}{N} \sum_{i=1}^{N} \mid \frac{y_{i}-\hat{y}_{i}}{y_{i}}$$

### RMSLE: root mean square logarithmic error

$$
\begin{aligned}
\operatorname{RMSLE} &=\sqrt{\frac{1}{N} \sum_{i=1}^{N}\left(\log \left(y_{i}+1\right)-\log \left(\hat{y}_{i}+1\right)\right)^{2}}=\\
&=\operatorname{RMSE}\left(\log \left(y_{i}+1\right), \log \left(\hat{y}_{i}+1\right)\right)=\\
&=\sqrt{\operatorname{MSE}\left(\log \left(y_{i}+1\right), \log \left(\hat{y}_{i}+1\right)\right)}
\end{aligned}
$$


## Classification metrics notation

$N-$ is number of objects

$L-$ is number of classes

$y-$ ground truth

$\hat{y}-$ predictions 

$[a=b]-$ indicator function

Soft labels (soft predictions) are classifier's scores

Hard labels (hard predictions):

Types of return:

**True Positives** (TP): should be TRUE, you predicted TRUE

**False Positives** (FP): should be FALSE, you predicted TRUE

**True Negative** (TN): should be FALSE, you predicted FALSE

**False Negatives** (FN): should be TRUE, you predicted FALSE 

### Accuracy

How frequently our class prediction is correct.

$\text { Accuracy }=\frac{1}{N} \sum_{i=1}^{N}\left[\hat{y}_{i}=y_{i}\right]$

$\text { Accuracy } =\large \frac{\text { TP }+\text { TN }}{\text { TP }+\text { FP }+\text { TN }+\text { FN }}$

### Logarithmic loss (logloss)

#### Binary

$$
\begin{array}{c}
\operatorname{LogLoss}=-\frac{1}{N} \sum_{i=1}^{N} y_{i} \log \left(\hat{y}_{i}\right)+\left(1-y_{i}\right) \log \left(1-\hat{y}_{i}\right) \\
y_{i} \in \mathbb{R}, \quad \hat{y}_{i} \in \mathbb{R}
\end{array}
$$

#### Multiclass:

$$
\begin{aligned}
\operatorname{LogLoss} &=-\frac{1}{N} \sum_{i=1}^{N} \sum_{i=1}^{L} y_{i l} \log \left(\hat{y}_{i l}\right) \\
y_{i} & \in \mathbb{R}^{L}, \quad \hat{y}_{i} \in \mathbb{R}^{L}
\end{aligned}
$$


### AUC (ROC)

ROC (receiver operating characteristic) curve is a performance measurement originally designed for the **binary classification problems**.

ROC curves typically feature **true positive** rate on the **Y** axis, and **false positive** rate on the **X** axis. 

The area under that ROC curve is AUC (area under the curve).

The Gini coefficient is comparable to the AUC score inasmuch as it also is a rank-based metric that can be used to measure how well your model is at discriminating TRUE from FALSE values.



![gini](/wp-content/uploads/2021/02/gini.png)


The ROC curve is obtained plotting model's true-positive and false-positive rates at different points.

> TP are true positives, FP are false positives

$\begin{aligned} \mathrm{AUC} &=\frac{\# \text { correctly ordered pairs }}{\text { total number of pairs }}=\\ &=1-\frac{\# \text { incorrectly ordered pairs }}{\text { total number of pairs }} \end{aligned}$

![auc](/wp-content/uploads/2021/02/auc.png)


To calculate the AUC from this image, we need to take care all the **pairs**, blue vs. orange dots.

Ideally, all blue dots should be on the left, and all orange dots on the right.

For the first orange dot there are two blue dots to the right, these are incorrectly ordered pairs.

Total number of pairs is 9 and correctly ordered paris is 7 so AUC = 7/9.

If there exists the **threshold** to separate all the dataset perfectly the AUC will be 1.

AUC metric doesn't take in account the absolute nor the relative values of the dots, all it cares is the order (which dot goes first).

To get the order we use the absolute values and **rank** them.

Precision $=\frac{\text { true positives }}{\text { true positives }+\text { false positives }}$

Recall $=\frac{\text { true positives }}{\text { true positives }+\text { false negatives }}$

$F_{1}=2 \cdot \frac{\text { Precision } \cdot \text { Recall }}{\text { Precision }+\text { Recall }}$

$F_{1}=\frac{2 \cdot \text { true positives }}{2 \cdot \text { true positives }+\text { false negatives }+\text { false positives }}$

![auc2](/wp-content/uploads/2021/02/auc2.png)


Positives and negatives are two sets of outcomes for a binary test. The blue curve shows distribution of negatives and the red curve shows distribution of positives. This distribution is obtained from the result of a classifier which estimates the probability of a test point being positive.

The key point to note is the area under curve (AUC) is the highest when the two curves are farthest with little overlap.

Adjust mean of negatives using slider. You can also drag the threshold line in the graph below. 


> With some modifications it may be used also for the [multiclass classification](https://en.wikipedia.org/wiki/Multiclass_classification){:rel="nofollow"} problems.



When to Use ROC vs. Precision-Recall Curves?

Generally, the use of ROC curves and precision-recall curves are as follows:

    ROC curves should be used when there are roughly equal numbers of observations for each class.
    Precision-Recall curves should be used when there is a moderate to large class imbalance.



### Quadratic Weighted Kappa 


...


## Review of Scikit learn metrics methods

### Classification


metrics | method |
---------|--------|
accuracy | `metrics.accuracy_score` |
balanced_accuracy | `metrics.balanced_accuracy_score`
top_k_accuracy| `metrics.top_k_accuracy_score`
average_precision| `metrics.average_precision_score`
neg_brier_score|`metrics.brier_score_loss`
f1| `metrics.f1_score`
neg_log_loss|`metrics.log_loss`
roc_auc|`metrics.roc_auc_score`
roc_auc_ovr|`metrics.roc_auc_score`
roc_auc_ovo|`metrics.roc_auc_score`
roc_auc_ovr_weighted|`metrics.roc_auc_score`
roc_auc_ovo_weighted|`metrics.roc_auc_score`


...


### Regression

metrics | method |
---------|--------|
explained_variance| `metrics.explained_variance_score`
max_error |`metrics.max_error`
neg_mean_absolute_error | `metrics.mean_absolute_error`
neg_mean_squared_error | `metrics.mean_squared_error`
neg_root_mean_squared_error |`metrics.mean_squared_error`
neg_mean_squared_log_error |`metrics.mean_squared_log_error`
neg_median_absolute_error|`metrics.median_absolute_error`
r2|`metrics.r2_score`
neg_mean_poisson_deviance|`metrics.mean_poisson_deviance`
neg_mean_gamma_deviance |`metrics.mean_gamma_deviance`
neg_mean_absolute_percentage_error`|`metrics.mean_absolute_percentage_error`

...


---
Reference:

https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/

---