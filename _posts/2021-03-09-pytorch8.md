---
id: 100
title: PyTorch | Optimizer
date: 2021-03-09 23:57:32
author: taimane
layout: post
permalink: /pytorch/optimizer
published: false
categories:
   - pytorch
tags:
   - optimizer
---
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

- [About PyTorch Optimizers](#about-pytorch-optimizers)
- [Creating a custom optimizer](#creating-a-custom-optimizer)

## About PyTorch Optimizers

PyTorch has default optimizers. Most famous is `torch.optim.SGD`, followed by `torch.optim.Adam` or 
`torch.optim.AdamW`.
>The original Adam algorithm was proposed in [Adam: A Method for Stochastic Optimization](href="https://arxiv.org/abs/1412.6980). The AdamW variant was proposed in [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).

Recently very popular is also `torch.optim.LBFGS` inspired by a matlab function [minFunc](https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html).


It is really easy to create custom PyTorch optimizer. This is just a Python class.
It need to have a constructor `__init__`, it need to have a state dict `__state_dict__` or the state.


## Creating a custom optimizer

