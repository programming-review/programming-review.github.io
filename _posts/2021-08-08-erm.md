---
id: 12971
title: ERM
date: 2020-04-01
author: taimane
layout: post
permalink: /machine-learning/erm
published: true
image: 
categories: 
   - machine-learning
tags:
   - data analysis
---
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


**Empirical risk minimization**  ERM is a concept in machine learning that aims to find the set of models with the best performance on a given problem. 

ERM is determined by how well the model predicts a given problem, and not by how well the model fits the data. 

ERM is based in statistical learning theory and fits well if we don't know the **true distribution** of data that the algorithm will work on.

In other words we don't know all the data and we just have a subset of the data called **the training data**.


## Limitation

As explained ERM works with the specific limitation. We measure the performance on a known set of training data only.


## Definition


Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true "risk") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the "empirical" risk). 
