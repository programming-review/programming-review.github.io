---
id: 12971
title: Bert
date: 2020-04-01
author: taimane
layout: post
permalink: /machine-learning/bert
published: false
image:
categories:
   - machine-learning
tags:
   - data analysis
   - bert
   - roberta
   - nlp
   - text classification
   - question answering
   - sentence pair classification
   - sequence tagging
   - masking 
   - sentence masking   
   - mask token
   - cls token
   - sep token
   - class label
   - fine tunning
   - model or hypothesis

---
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
 
_Table of Contents:_
 
BERT is Bidirectional Encoder Transormer. It is the transformer architecture specific advancement. 


BERT relay on 3 kind of embeddings:

* Token embeddings
* Segment embeddings
* Positional embeddings

_All the embeddings are somehow combined._

BERT tasks:

* Masked Language Modeling
* Next Sentence Prediction

Masked language modeling means masking 15% of tokens from the sentence in a very specific way:
* 80% of these 15% are replaced with the `[mask]` token
* 10 percent are left intact
* 10 percent are just replaced with some random token

Nest sentence prediction works this way. 50% of cases we set the real follower sentence, else we provide the random sentence from the corpus.

Just these two tasks made BERT in 2019 **state of the art** (SOTA) on the NLP GLUE task.

Apparently later BERT Next sentence prediction task was removed in a model called Roberta since they found out next sentence prediction task haven't contributed much to the model quality.

## BERT and 512

What is the meaning of 512 in BERT? 
It is maximum sequence length. This means how many tokens you may have when providing the input to the BERT.


## BERT may be used for ...

BERT may be used for the following tasks:

* Sequence classification
* Sentence pair classification
* Question answering
* Sequence tagging

Sentence classification can be either: binary, multiclass or multilabel.

Sentence pair classification can answer the question if the second sentence is related (follows) to the first one.

Question answering can determine the most probable answer to our question (first sentence) based on the text input (second sentence).

BERT question answering model can highlight the paragraph where the answer is contained and more specifically it can point exactly to an answer.

Of course BERT should be fine tuned for the question answering task first.

Sequence tagging is used for NER (named entity recognition) and can help use determine type of the word (token): name, verb, location, etc.

